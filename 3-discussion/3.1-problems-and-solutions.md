# 3.1 Problems & Solutions

In this section, we will discuss some of the probable problems in the Covenant consensus and the practical solutions that have been considered for the protocol implementation.&#x20;

### Problem: Leader Failover

The Prime node is a single-point-of-failure in the system, so node failover must be implemented to ensure availability. In Covenant, this failover is called a Prime election. The general idea is that when the Prime node is alive and reachable, it sends out heartbeat messages to all followers every, say, 50ms (configurable). When the leader is unreachable (dead or network outage), some followers will send out messages to peers and call for an election. Detailed information is available in Section [1.4](../1-hybrid-bft/1.4-faulty-prime-node-elimination.md).

### Problem: PRC Definition

From the descriptions above, it seems that there are many types of messages between Initium nodes — broadcast commands, heartbeats, run elections, commit logs, etc. However, Covenant is designed so carefully that it uses only **two** RPCs:

**1) AppendEntry**: This RPC is initiated by the Prime node and carries the latest commands received. It also serves as the heartbeat message. When a follower gets this message, the election timer is reset.&#x20;

2\) **RequestVote**: When a follower’s election timer goes off (No AppendEntry received for a long time, \~100ms), it becomes a candidate and calls all other nodes to vote for itself. The others can either grant or decline their votes. Once the majority says yes, the candidate becomes the Prime. Other candidates revert to Supervisors.

### Problem: Multiple Elections

Without any election coordination, nothing can stop Initium nodes from running multiple elections. Since each node votes for itself (by design), it is possible that none of the elections can gather majority votes. Without any consensus, the nodes are stuck in the election loop again and again without a leader. To avoid concurrent elections, Covenant ventures for a simple solution — a randomized timeout. The key idea here is that randomization reduces the probability of election collision. If a split vote happens, we can simply retry with a very large probability of getting it done the next round.

### Problem: Multiple Leaders

Can multiple Prime nodes even exist in an Initium cluster with strict majority votes? The answer is both yes and no. If a Prime $$p_1$$ is cut off from all other nodes, a new election will happen amongst the followers, and eventually, a new leader is elected ($$p_2$$). For $$p_1$$, since it has no way of detecting the presence of $$p_2$$, it still functions as the Prime node. So yes, there can be multiple leaders in Covenant. However, if $$p_1$$ cannot reach the majority, it cannot handle any message $$M$$ requests and cannot form any connection with Supervisor nodes. So, from the perspective of the client, $$p_1$$ is as good as dead. So no, only one leader ($$p_2$$) can be in service. A comprehensive answer to this problem can be found in Section [1.3](../1-hybrid-bft/1.3-consensus-flow.md).&#x20;
